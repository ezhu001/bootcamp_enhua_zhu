{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6a69da8-df3e-493e-9770-cf5eb2ac8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13466\\AppData\\Local\\Temp\\ipykernel_41584\\3412681335.py:64: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Date       Close        High         Low        Open    Volume\n",
      "Ticker                   AAPL        AAPL        AAPL        AAPL      AAPL\n",
      "0      2025-08-14  232.779999  235.119995  230.850006  234.059998  51916300\n",
      "1      2025-08-15  231.589996  234.279999  229.339996  234.000000  56010500\n",
      "Saved CSV to data\\raw\\api_yfinance_AAPL_20250816-1726_2days.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#API Usage\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#Finding path for saving files\n",
    "DATA_RAW = Path(\"./data/raw\")\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Ticker and API setups\n",
    "ticker = \"AAPL\"\n",
    "now = datetime.now()\n",
    "timeformat = now.strftime(\"%Y%m%d-%H%M\")\n",
    "load_dotenv()\n",
    "FINNHUB_KEY = os.getenv(\"FINNHUB_API\") \n",
    "#Alert if needed\n",
    "if not FINNHUB_KEY:\n",
    "    raise RuntimeError(\"Please set your FINNHUB_KEY environment variable.\")\n",
    "#Time setup - 2 days of financial data for Apple\n",
    "now = int(time.time())\n",
    "two_days_ago = now - 2*24*60*60\n",
    "#Try to use FINNHUB first (actually it will not work, as even 2days of data require subscription)\n",
    "#However, same-day data is within the free-tier\n",
    "#But I can easily replace it with yfinance\n",
    "try: \n",
    "    source = \"finnhub\"\n",
    "    url = \"https://finnhub.io/api/v1/stock/candle\"\n",
    "    params = {\n",
    "        \"ticker\": ticker,\n",
    "        \"resolution\": \"D\", \n",
    "        \"from\": two_days_ago,\n",
    "        \"to\": now,\n",
    "        \"token\": FINNHUB_KEY\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    \n",
    "    if js.get(\"s\") != \"ok\":\n",
    "        raise ValueError(f\"Unexpected response: {js}\")\n",
    "    \n",
    "    # build dataframe\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": pd.to_datetime(js[\"t\"], unit=\"s\"),\n",
    "        \"open\": js[\"o\"],\n",
    "        \"high\": js[\"h\"],\n",
    "        \"low\": js[\"l\"],\n",
    "        \"close\": js[\"c\"],\n",
    "        \"volume\": js[\"v\"],\n",
    "    })\n",
    "    \n",
    "    print(df)\n",
    "#In the case of FINNHUB not working, I will be using yfinance\n",
    "except:\n",
    "    source = \"yfinance\"\n",
    "    end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    start_date = (datetime.today() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Download data\n",
    "    df = yf.download(ticker, start=start_date, end=end_date) \n",
    "    \n",
    "    # Reset index to make DateTime a column\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    print(df.head())\n",
    "\n",
    "#Save to CSV in the end\n",
    "output_file = DATA_RAW / f\"api_{source}_{ticker}_{timeformat}_2days.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved CSV to {output_file}\")\n",
    "\n",
    "#Eventually, yfinance (instead of FINNHUB) produced the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40084bbd-68d9-4cd3-a16a-41eb285c74bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to data\\raw\\largest_companies_scraped.csv\n",
      "               Ranks                             Name  \\\n",
      "0  USD (in millions)                                    \n",
      "1                  1                          Walmart   \n",
      "2                  2                           Amazon   \n",
      "3                  3  State Grid Corporation of China   \n",
      "4                  4                     Saudi Aramco   \n",
      "\n",
      "                       Industry   Revenue    Profit  Employees  \\\n",
      "0                                                          NaN   \n",
      "1                        Retail  $680,985   $19,436  2100000.0   \n",
      "2  Retailinformation technology  $637,959   $59,248  1556000.0   \n",
      "3                   Electricity  $545,948    $9,204  1361423.0   \n",
      "4                   Oil and gas  $480,446  $106,246    73311.0   \n",
      "\n",
      "  Headquarters[note 1] State-owned Ref.  \n",
      "0                                        \n",
      "1        United States              [1]  \n",
      "2                              [4]       \n",
      "3                China              [5]  \n",
      "4         Saudi Arabia              [6]  \n"
     ]
    }
   ],
   "source": [
    "#Scraping\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#Finding path for saving files similarly\n",
    "DATA_RAW = Path(\"data/raw\")\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Fetching (in this part I kind of asked for help from GPT to let me know what are\n",
    "#good websites for scraping data, as I am not sure of that)\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}  \n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()  # will raise error if request failed\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "#I used some GPT to assist me in scraping efficiently and in a stable way\n",
    "#Otherwise I really do not know how to scrape the right table\n",
    "header_row = table.find(\"tr\")\n",
    "columns = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "#Extracting data\n",
    "rows = table.find_all(\"tr\")[1:] \n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all([\"th\", \"td\"])\n",
    "    cols_text = [c.get_text(strip=True) for c in cols]\n",
    "    # Pad or truncate row to match header length\n",
    "    if len(cols_text) < len(columns):\n",
    "        cols_text += [\"\"] * (len(columns) - len(cols_text))\n",
    "    elif len(cols_text) > len(columns):\n",
    "        cols_text = cols_text[:len(columns)]\n",
    "    data.append(cols_text)\n",
    "\n",
    "#Building dataframe and columns for storing data\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "numeric_cols = [\"Revenue in USD billions\", \"Profit in USD billions\", \"Employees\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col].str.replace(\",\", \"\").str.replace(\"â€“\", \"\"), errors=\"coerce\")\n",
    "\n",
    "#Save to CSV in the end\n",
    "output_file = DATA_RAW / \"largest_companies_scraped.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved CSV to {output_file}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203ebb3-c1ed-4e1c-bf5e-c9ac4dd1b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API\n",
    "#Data source: FINNHUB or Yahoo Finance\n",
    "#URL: https://finnhub.io/api/v1/stock/candle or https://finance.yahoo.com/quote/AAPL/history/\n",
    "#params: Date, Close, High, Low, Open, Volume\n",
    "#Potential problems include: \n",
    "#1. subscription might be needed in future (like FINNHUB)\n",
    "#2. URL might change and require updates\n",
    "#3. while API-formatted data sources usually have more consistent format, \n",
    "# change of format/rubrics might still be a risk\n",
    "\n",
    "#Scraping\n",
    "#Data source: Wikipedia\n",
    "#URL: https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue\n",
    "#params: Ranks, Name, Industry, Revenue, Profit, Employees, Headquarters, State-owned, Ref.\n",
    "#Potential problems include: \n",
    "#1. Inconsistent format and might consistently require the programmer to modify codes\n",
    "#2. Potential change in policies regarding scraping\n",
    "#3. The extent to which the data is allowed to be used might be unclear or not as clearly stated as most commercial API data sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
